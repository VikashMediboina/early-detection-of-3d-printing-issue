{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTImageProcessor, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"/Users/vikashmediboina/Downloads/early-detection-of-3d-printing-issues/test.csv\")\n",
    "test_data[\"img_path\"]= [\"/Users/vikashmediboina/Downloads/early-detection-of-3d-printing-issues/images/\" + each for each in test_data[\"img_path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>printer_id</th>\n",
       "      <th>print_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/vikashmediboina/Downloads/early-detecti...</td>\n",
       "      <td>101</td>\n",
       "      <td>1678578332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/vikashmediboina/Downloads/early-detecti...</td>\n",
       "      <td>101</td>\n",
       "      <td>1678578332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/vikashmediboina/Downloads/early-detecti...</td>\n",
       "      <td>101</td>\n",
       "      <td>1678578332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/vikashmediboina/Downloads/early-detecti...</td>\n",
       "      <td>101</td>\n",
       "      <td>1678578332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/vikashmediboina/Downloads/early-detecti...</td>\n",
       "      <td>101</td>\n",
       "      <td>1678578332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  printer_id    print_id\n",
       "0  /Users/vikashmediboina/Downloads/early-detecti...         101  1678578332\n",
       "1  /Users/vikashmediboina/Downloads/early-detecti...         101  1678578332\n",
       "2  /Users/vikashmediboina/Downloads/early-detecti...         101  1678578332\n",
       "3  /Users/vikashmediboina/Downloads/early-detecti...         101  1678578332\n",
       "4  /Users/vikashmediboina/Downloads/early-detecti...         101  1678578332"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(pth_file_path)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Access the model parameters or other saved data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer_state_dict \u001b[38;5;241m=\u001b[39m loaded_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ... (access other saved data as needed)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the path to the .pth file\n",
    "pth_file_path = 'modelvision.pth'\n",
    "\n",
    "# Load the .pth file\n",
    "loaded_model = torch.load(pth_file_path)\n",
    "\n",
    "# Access the model parameters or other saved data\n",
    "model_state_dict = loaded_model['model_state_dict']\n",
    "optimizer_state_dict = loaded_model['optimizer_state_dict']\n",
    "# ... (access other saved data as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "supported devices include CPU, CUDA and HPU, however got MPS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPS_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the PyTorch model with MPS support\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViTmodel.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Run the model on input data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/jit/_serialization.py:162\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files, _restore_shapes)\u001b[0m\n\u001b[1;32m    160\u001b[0m cu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mCompilationUnit()\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath)):\n\u001b[0;32m--> 162\u001b[0m     cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mimport_ir_module(cu, \u001b[39mstr\u001b[39;49m(f), map_location, _extra_files, _restore_shapes)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mimport_ir_module_from_buffer(\n\u001b[1;32m    165\u001b[0m         cu, f\u001b[39m.\u001b[39mread(), map_location, _extra_files, _restore_shapes\n\u001b[1;32m    166\u001b[0m     )  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: supported devices include CPU, CUDA and HPU, however got MPS"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Set up the MPS environment variables\n",
    "os.environ['LD_PRELOAD'] = '/usr/local/cuda/extras/CUPTI/lib64/libcupti.so:/usr/local/cuda/targets/x86_64-linux/lib/libnvidia-ml.so'\n",
    "os.environ['MPS_SERVER'] = 'localhost:53331'\n",
    "os.environ['MPS_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Load the PyTorch model with MPS support\n",
    "loaded_model = torch.jit.load(\"ViTmodel.pt\")\n",
    "\n",
    "# Run the model on input data\n",
    "input_data = torch.randn(1, 3, 224, 224)\n",
    "output = loaded_model(input_data)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "supported devices include CPU, CUDA and HPU, however got MPS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViTmodel.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# loaded_model.()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model on input data\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/jit/_serialization.py:162\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files, _restore_shapes)\u001b[0m\n\u001b[1;32m    160\u001b[0m cu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mCompilationUnit()\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath)):\n\u001b[0;32m--> 162\u001b[0m     cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mimport_ir_module(cu, \u001b[39mstr\u001b[39;49m(f), map_location, _extra_files, _restore_shapes)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mimport_ir_module_from_buffer(\n\u001b[1;32m    165\u001b[0m         cu, f\u001b[39m.\u001b[39mread(), map_location, _extra_files, _restore_shapes\n\u001b[1;32m    166\u001b[0m     )  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: supported devices include CPU, CUDA and HPU, however got MPS"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load(\"modelvision.pth\",map_location=torch.device('cpu'))\n",
    "input_data = torch.randn(16,3,224,224).to(device)\n",
    "# loaded_model.()\n",
    "# Evaluate the model on input data\n",
    "trained_model = loaded_model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch-mps (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch-mps\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch-mps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 14, in forward\n    input = torch.to((modality1).forward(input1, ), dtype=6, layout=0, device=torch.device(\"mps\"))\n    x = torch.to(torch.flatten(input, 1), dtype=6, layout=0, device=torch.device(\"mps\"))\n    _0 = torch.to((head).forward(x, ), dtype=6, layout=0, device=torch.device(\"mps\"))\n                   ~~~~~~~~~~~~~ <--- HERE\n    return _0\nclass MLP(Module):\n  File \"code/__torch__.py\", line 27, in forward\n    _1 = ops.prim.NumToTensor(torch.size(x, 0))\n    input = torch.view(x, [int(_1), -1])\n    return (layers).forward(input, )\n            ~~~~~~~~~~~~~~~ <--- HERE\n  File \"code/__torch__/torch/nn/modules/container.py\", line 53, in forward\n    _1 = getattr(self, \"1\")\n    _0 = getattr(self, \"0\")\n    _14 = (_1).forward((_0).forward(input, ), )\n                        ~~~~~~~~~~~ <--- HERE\n    _15 = (_4).forward((_3).forward((_2).forward(_14, ), ), )\n    _16 = (_7).forward((_6).forward((_5).forward(_15, ), ), )\n  File \"code/__torch__/torch/nn/modules/linear/___torch_mangle_209.py\", line 12, in forward\n    bias = self.bias\n    weight = self.weight\n    return torch.linear(input, weight, bias)\n           ~~~~~~~~~~~~ <--- HERE\n\nTraceback of TorchScript, original code (most recent call last):\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py(114): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/container.py(217): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\nTransformer.py(52): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\nTransformer.py(81): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/jit/_trace.py(1056): trace_module\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/jit/_trace.py(794): trace\nTransformer.py(172): <module>\nRuntimeError: Placeholder storage has not been allocated on MPS device!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert the model to CPU and evaluate\u001b[39;00m\n\u001b[1;32m     13\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 14, in forward\n    input = torch.to((modality1).forward(input1, ), dtype=6, layout=0, device=torch.device(\"mps\"))\n    x = torch.to(torch.flatten(input, 1), dtype=6, layout=0, device=torch.device(\"mps\"))\n    _0 = torch.to((head).forward(x, ), dtype=6, layout=0, device=torch.device(\"mps\"))\n                   ~~~~~~~~~~~~~ <--- HERE\n    return _0\nclass MLP(Module):\n  File \"code/__torch__.py\", line 27, in forward\n    _1 = ops.prim.NumToTensor(torch.size(x, 0))\n    input = torch.view(x, [int(_1), -1])\n    return (layers).forward(input, )\n            ~~~~~~~~~~~~~~~ <--- HERE\n  File \"code/__torch__/torch/nn/modules/container.py\", line 53, in forward\n    _1 = getattr(self, \"1\")\n    _0 = getattr(self, \"0\")\n    _14 = (_1).forward((_0).forward(input, ), )\n                        ~~~~~~~~~~~ <--- HERE\n    _15 = (_4).forward((_3).forward((_2).forward(_14, ), ), )\n    _16 = (_7).forward((_6).forward((_5).forward(_15, ), ), )\n  File \"code/__torch__/torch/nn/modules/linear/___torch_mangle_209.py\", line 12, in forward\n    bias = self.bias\n    weight = self.weight\n    return torch.linear(input, weight, bias)\n           ~~~~~~~~~~~~ <--- HERE\n\nTraceback of TorchScript, original code (most recent call last):\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py(114): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/container.py(217): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\nTransformer.py(52): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\nTransformer.py(81): forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/jit/_trace.py(1056): trace_module\n/Users/vikashmediboina/mambaforge/envs/printing3.8/lib/python3.8/site-packages/torch/jit/_trace.py(794): trace\nTransformer.py(172): <module>\nRuntimeError: Placeholder storage has not been allocated on MPS device!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Set environment variables for MPS\n",
    "os.environ['MPS_VERBOSE'] = '1'\n",
    "os.environ['MPS_MAX_LINES'] = '3'\n",
    "os.environ['MPS_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Load the PyTorch model with MPS support\n",
    "model_script = torch.jit.script(torch.load('ViTmodel.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "# Convert the model to CPU and evaluate\n",
    "input_data = torch.randn(16, 3, 224, 224).to('cpu')\n",
    "trained_model = model_script(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTModel,ViTConfig\n",
    "\n",
    "# Load the saved state dictionary\n",
    "loaded_state_dict = torch.load('ViTmodel.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "\n",
    "# Convert the loaded model to a state dictionary\n",
    "loaded_state_dict = loaded_model.state_dict()\n",
    "\n",
    "# Create a new model instance with the same configuration\n",
    "config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "new_model = ViTModel(config)\n",
    "\n",
    "# Load the state dictionary into the new model instance\n",
    "new_model.load_state_dict(loaded_state_dict, strict=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "new_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.2293, -0.1140,  0.0780,  ..., -0.1330, -0.0726, -0.1230],\n",
      "         [ 0.2210, -0.3239,  0.2153,  ..., -0.1833, -0.0287, -0.0542],\n",
      "         [ 0.3217, -0.1437,  0.1398,  ..., -0.0003, -0.1282, -0.0058],\n",
      "         ...,\n",
      "         [ 0.1537, -0.1601,  0.0014,  ..., -0.0038, -0.2342, -0.0676],\n",
      "         [ 0.1608, -0.1199, -0.0024,  ...,  0.0550, -0.0499, -0.2255],\n",
      "         [ 0.2512, -0.0451,  0.1070,  ...,  0.0008, -0.0839, -0.0868]]]), pooler_output=tensor([[ 4.2619e-01, -1.3044e-01, -7.9687e-01, -5.4830e-02, -6.7148e-01,\n",
      "          3.2121e-02,  4.1386e-01,  1.7094e-01,  1.9688e-01, -3.8563e-01,\n",
      "          5.3947e-01,  1.5896e-01, -2.0352e-01,  3.7294e-01, -2.7611e-01,\n",
      "         -3.1651e-01, -3.9442e-01, -4.5078e-01,  3.7674e-01,  5.7867e-01,\n",
      "          7.8278e-01, -1.3906e-01,  2.2333e-01,  3.2622e-01,  4.9351e-01,\n",
      "         -3.2776e-01, -4.1125e-02,  5.4864e-02,  1.2116e-01,  1.5514e-01,\n",
      "         -5.9572e-01,  7.3256e-01, -5.1687e-01,  4.6378e-01,  1.9732e-01,\n",
      "          7.7353e-02, -3.7761e-02, -4.4169e-01,  1.0375e-02, -6.0767e-02,\n",
      "          5.4333e-01, -2.5031e-01,  8.9028e-01,  3.0280e-01,  2.5001e-02,\n",
      "         -5.4604e-01, -2.5512e-01,  1.9249e-01, -2.8242e-01,  1.0189e-01,\n",
      "         -8.7449e-02, -3.6560e-01, -6.6567e-01, -1.3620e-01,  1.0920e-01,\n",
      "          1.2736e-01, -4.9619e-01, -3.8212e-01, -2.7504e-01, -1.5444e-01,\n",
      "          7.4689e-01, -1.4768e-01,  4.8962e-01, -4.0008e-01,  6.0756e-01,\n",
      "         -6.4525e-01,  3.1962e-01,  3.9277e-01,  7.4302e-01,  1.2975e-02,\n",
      "          3.0073e-01,  1.3440e-01,  4.4086e-01, -2.6767e-01,  7.7902e-03,\n",
      "         -4.9279e-01, -8.8652e-02,  3.3723e-01, -6.1058e-03, -3.6369e-01,\n",
      "          2.6926e-02, -7.2350e-01, -1.5719e-01,  3.3304e-01,  3.0077e-02,\n",
      "         -1.8372e-01, -9.6337e-02,  1.8900e-01,  6.4373e-01,  1.1222e-01,\n",
      "         -4.6512e-01, -3.5478e-01, -5.6739e-01, -5.3976e-01,  3.8607e-01,\n",
      "          2.4281e-01, -2.1827e-01,  4.7004e-01, -4.2550e-01, -4.6071e-01,\n",
      "         -5.8597e-01, -6.1619e-01,  8.4263e-02, -2.4377e-01,  7.9556e-01,\n",
      "          1.6786e-01, -3.5787e-01,  4.3813e-01,  3.9308e-02,  1.5426e-01,\n",
      "          1.5460e-01,  6.0328e-01, -9.8286e-02, -4.6113e-01,  2.4532e-01,\n",
      "          7.1478e-01,  2.2096e-01, -7.2110e-01,  6.1540e-01,  3.2317e-01,\n",
      "          1.6451e-01,  9.3729e-02, -3.8219e-01, -7.1545e-01,  2.5644e-01,\n",
      "         -5.0353e-01,  2.2077e-01,  5.1832e-01, -2.2319e-01, -3.3480e-02,\n",
      "         -8.1124e-02,  2.6735e-01, -1.3792e-01,  5.4018e-01,  1.1441e-01,\n",
      "          5.0673e-01, -2.5833e-01, -1.5870e-05,  2.2575e-01, -2.4792e-01,\n",
      "         -2.3840e-01,  4.3980e-02,  5.5133e-01, -8.5663e-01, -3.9958e-01,\n",
      "         -5.0053e-01, -1.3856e-01, -5.5160e-01, -6.6772e-01, -2.2993e-01,\n",
      "         -5.0467e-02, -4.5195e-01,  3.0737e-01, -1.4067e-01, -1.0240e-01,\n",
      "         -6.4096e-01, -2.6519e-01, -5.3956e-01,  4.5087e-01, -8.8212e-02,\n",
      "          3.9510e-01, -7.2299e-01,  1.5224e-02, -4.2498e-01,  3.6509e-01,\n",
      "         -1.7704e-02, -3.2528e-01,  1.3462e-02, -7.0899e-01, -7.8333e-02,\n",
      "         -2.4015e-02,  1.7651e-01, -3.1221e-02,  4.2110e-01, -4.6259e-02,\n",
      "          3.3454e-01, -4.5350e-02,  1.3974e-01,  6.2541e-01,  6.7691e-01,\n",
      "         -1.9316e-01,  4.8529e-01, -5.8801e-01,  7.2278e-01,  3.0314e-01,\n",
      "          7.0729e-01,  4.1940e-01,  5.8191e-01, -6.7805e-01, -4.9859e-01,\n",
      "         -7.0472e-01,  3.6787e-01, -2.7849e-01, -3.0732e-01,  7.9110e-01,\n",
      "          6.2664e-01, -6.3407e-01,  4.5600e-01,  3.0009e-01,  2.9208e-01,\n",
      "          4.0350e-01, -1.2158e-01,  6.7897e-01,  3.6800e-01, -2.2996e-01,\n",
      "          5.9133e-01, -3.5601e-01, -7.0258e-01,  5.6589e-04,  5.2837e-01,\n",
      "          1.4562e-01, -1.4840e-01, -3.3164e-02, -1.0962e-01,  2.9452e-01,\n",
      "         -4.0245e-01,  7.5882e-01, -1.5015e-01, -2.6733e-01, -3.9965e-03,\n",
      "         -1.2417e-01,  6.4405e-02,  3.3243e-01, -3.2207e-01,  7.4562e-01,\n",
      "         -1.0282e-01,  4.3181e-02, -4.8824e-01, -5.4210e-01,  5.3017e-01,\n",
      "         -4.5659e-01, -5.1596e-01, -6.4358e-01, -1.6698e-01,  2.3609e-01,\n",
      "          3.7449e-01,  9.5165e-02, -4.9347e-01,  3.6691e-01,  1.3355e-01,\n",
      "         -1.0725e-01, -2.7063e-01,  2.7782e-01, -1.9786e-01,  4.9380e-01,\n",
      "          6.1436e-01, -7.0849e-01,  3.7553e-01,  2.5184e-01,  5.2305e-01,\n",
      "          1.8632e-01, -1.3219e-02, -2.3080e-01, -2.7301e-03, -4.3608e-01,\n",
      "          4.2538e-01,  7.6738e-01, -8.9675e-01, -6.0824e-01,  2.4147e-01,\n",
      "          4.9597e-01,  5.0321e-02,  3.9051e-01,  1.5848e-01,  1.5416e-01,\n",
      "         -1.9742e-01,  4.8474e-01, -4.8414e-01, -4.1065e-01, -1.0766e-02,\n",
      "          3.9082e-03,  3.3172e-01, -4.6699e-01,  1.9614e-01, -5.0746e-02,\n",
      "         -5.5691e-02, -3.3237e-01, -3.4716e-02,  4.1390e-02,  5.9197e-01,\n",
      "          1.9401e-02,  5.8219e-01,  6.0568e-01, -6.1104e-01, -4.0982e-01,\n",
      "          1.0277e-01, -2.7986e-01,  4.9511e-01, -6.8435e-01,  3.8090e-01,\n",
      "          2.6040e-01, -5.5950e-01,  5.8243e-02,  6.8672e-02, -3.3565e-01,\n",
      "          4.7170e-02, -7.9430e-02,  1.8798e-01, -2.7541e-01,  4.1346e-03,\n",
      "          2.8704e-01,  1.8662e-01, -7.7049e-02,  3.5717e-01,  2.1198e-01,\n",
      "          2.7186e-01, -4.9644e-01,  3.1443e-01,  1.5399e-01, -3.1045e-01,\n",
      "          2.8288e-01,  2.1654e-01,  1.9235e-01, -2.5981e-01, -2.4585e-01,\n",
      "          2.1702e-01,  3.0167e-01, -2.4904e-01, -2.0810e-01,  3.1216e-01,\n",
      "          3.9232e-01, -4.8497e-02,  5.9967e-01, -3.9663e-01, -5.2838e-01,\n",
      "         -6.9132e-01,  5.9197e-01, -1.6865e-01, -6.1208e-01,  3.8104e-01,\n",
      "          2.6847e-01, -5.2953e-01,  5.3330e-01,  1.3922e-01, -1.1856e-01,\n",
      "          8.0132e-01, -5.9126e-01,  1.2642e-01, -1.1670e-01,  3.1876e-01,\n",
      "          5.1171e-01,  4.8271e-01, -6.3742e-01,  2.4433e-01, -2.4766e-01,\n",
      "          6.6737e-01, -1.8882e-01,  3.1792e-01, -3.7379e-01, -5.8803e-01,\n",
      "         -1.4030e-01,  3.5328e-01,  5.9145e-01, -2.1319e-01,  1.7482e-01,\n",
      "         -3.8743e-01,  1.6984e-02,  7.1919e-02,  2.3724e-01, -4.8006e-01,\n",
      "          5.9870e-02,  2.1381e-01,  2.3218e-02, -2.0897e-01, -8.5318e-02,\n",
      "          5.7270e-01,  1.1011e-01,  9.6470e-02, -4.7260e-03, -3.9807e-01,\n",
      "          4.0564e-01,  2.7071e-01,  2.8968e-02,  1.1908e-01, -2.5695e-01,\n",
      "         -5.7595e-01,  2.0139e-01,  2.8511e-01, -1.5909e-01,  5.4905e-01,\n",
      "          5.3443e-01,  4.8281e-01,  1.1251e-01, -1.7905e-01,  5.2719e-01,\n",
      "          3.8496e-01, -5.4222e-02, -8.8625e-02, -2.7527e-01,  1.1274e-01,\n",
      "          9.3720e-02,  2.0590e-01,  1.6940e-01,  1.0671e-01, -3.3364e-01,\n",
      "          2.7040e-01, -5.3097e-01,  5.5401e-01,  4.0817e-01, -5.4413e-01,\n",
      "          3.4371e-01,  1.8727e-01,  6.1051e-01,  3.0579e-01,  7.2638e-01,\n",
      "         -4.4364e-01, -9.0053e-01, -3.5441e-01, -7.9336e-01,  4.9781e-01,\n",
      "          1.7755e-01, -1.4757e-01, -2.4567e-01, -1.6560e-02, -4.2405e-01,\n",
      "          4.9318e-01,  3.2984e-01, -7.8776e-02, -4.0210e-01,  5.3833e-02,\n",
      "          5.2258e-01,  6.8451e-01, -2.5088e-01,  5.6466e-02,  5.7208e-01,\n",
      "         -3.1777e-02, -3.6461e-01,  1.4364e-01, -1.2064e-01,  3.3548e-01,\n",
      "          5.4941e-03, -4.5182e-01, -4.1381e-01, -1.1922e-01, -4.7689e-01,\n",
      "          1.3013e-01,  2.9038e-01,  6.0448e-01,  7.3481e-02,  2.0118e-01,\n",
      "         -2.0058e-01,  7.0818e-02, -8.3572e-02, -4.7447e-01, -1.7357e-01,\n",
      "          5.2824e-02,  6.7589e-01,  2.2642e-01, -5.0247e-01, -4.3582e-02,\n",
      "          5.6580e-01, -1.9377e-01, -5.8154e-02,  2.7597e-01, -2.0095e-01,\n",
      "          3.1522e-01,  6.6928e-01, -3.9487e-01,  7.5551e-02, -4.4658e-01,\n",
      "          6.9329e-01, -8.2939e-02, -7.4122e-01,  3.7690e-02,  3.9133e-01,\n",
      "          2.5093e-01,  1.1012e-01,  5.2754e-01,  5.7029e-01, -5.0734e-01,\n",
      "          6.5965e-01, -2.6212e-01, -2.4813e-01,  2.5790e-01,  2.4434e-01,\n",
      "          4.6813e-02,  1.9569e-01, -5.9691e-02,  8.1067e-01,  9.8566e-02,\n",
      "          5.8111e-01, -3.2075e-02, -5.5621e-01, -6.7953e-01,  8.8075e-02,\n",
      "         -6.2460e-01, -1.5501e-01, -7.8939e-01,  3.9145e-01,  7.1642e-01,\n",
      "         -5.3679e-02, -2.8367e-01, -6.1814e-01, -1.8051e-01,  3.4730e-01,\n",
      "         -2.4131e-02,  3.9015e-01,  4.4681e-01, -2.2711e-01, -5.3379e-01,\n",
      "          8.6913e-01, -2.0262e-01, -9.4728e-02, -3.0232e-01,  6.4110e-01,\n",
      "          3.4195e-01,  6.9568e-02, -1.9915e-01,  2.1161e-01,  1.7192e-01,\n",
      "          1.6313e-01,  2.3676e-01,  8.5305e-02,  2.2166e-01,  6.4290e-01,\n",
      "         -3.2579e-01,  1.3521e-01, -8.5692e-02,  5.8561e-01, -1.4519e-01,\n",
      "          4.6270e-01, -3.6221e-01, -4.1678e-01,  5.8054e-01, -3.7022e-01,\n",
      "          1.1612e-01,  4.2168e-01, -1.1878e-01, -3.0031e-01, -1.2346e-01,\n",
      "         -2.5405e-01, -3.4025e-01,  5.9307e-01, -2.5175e-01,  4.6195e-01,\n",
      "         -2.1790e-01, -7.1049e-03, -3.6347e-01, -5.9701e-01,  3.4179e-01,\n",
      "         -3.5701e-01,  6.7932e-01, -3.5887e-01, -7.4979e-02,  3.4974e-01,\n",
      "         -1.5404e-01, -1.8248e-01,  5.8919e-01,  4.0025e-01, -5.1339e-01,\n",
      "          1.8190e-01, -1.0395e-01, -2.6357e-01,  5.7340e-01,  4.6991e-02,\n",
      "          4.1815e-01, -3.0096e-01, -4.1205e-01, -8.9443e-02, -9.0036e-02,\n",
      "          4.1710e-01,  2.0935e-01,  9.5643e-02,  3.4921e-01, -4.2748e-01,\n",
      "          8.1658e-01, -3.5955e-01,  2.2576e-01, -9.3416e-02,  2.5728e-01,\n",
      "          1.6146e-01,  4.6028e-02, -5.2597e-01,  6.9659e-01, -3.2650e-01,\n",
      "          4.1992e-01, -5.5670e-01, -5.1428e-01, -2.4460e-01,  1.5883e-01,\n",
      "          2.1702e-01,  2.6884e-01, -7.6111e-02,  3.3287e-01, -1.6161e-01,\n",
      "         -4.8211e-01, -7.8240e-01, -2.1127e-01,  4.0621e-01,  3.1514e-01,\n",
      "          1.5033e-01,  1.4124e-01,  3.3344e-01,  3.8366e-01,  1.6759e-01,\n",
      "         -1.7169e-01,  3.8736e-01,  2.1882e-01,  6.5042e-01,  4.8369e-01,\n",
      "          1.2495e-01,  1.1418e-01, -2.2461e-01, -5.1867e-01,  3.3985e-01,\n",
      "         -7.8232e-02,  2.0475e-01,  5.3368e-01, -3.2922e-01,  7.1561e-01,\n",
      "          1.7408e-01,  7.2367e-01, -1.1488e-01, -4.6229e-02, -4.5209e-01,\n",
      "         -2.0637e-01, -3.4811e-01, -3.4596e-01,  1.4549e-01, -1.0360e-01,\n",
      "          2.3226e-02, -3.5213e-01, -2.9252e-01, -4.6440e-01, -5.4067e-02,\n",
      "         -2.0655e-01,  4.5947e-01, -3.7852e-01,  4.7382e-01, -5.7956e-02,\n",
      "          5.9687e-01, -4.0805e-01, -9.3408e-01,  4.4861e-01,  4.5449e-01,\n",
      "          4.1291e-01, -7.4488e-01,  7.7773e-02,  4.8005e-02, -4.0984e-01,\n",
      "          3.6898e-01,  1.6637e-01,  2.5416e-01, -3.1428e-01, -4.2163e-01,\n",
      "         -4.9203e-01,  8.6691e-01, -5.5515e-02,  2.1016e-01, -5.3005e-01,\n",
      "          9.5885e-02, -2.4439e-01, -5.3014e-02, -8.2935e-02, -1.3469e-01,\n",
      "         -1.1444e-01, -8.4937e-01, -5.4682e-02, -5.7625e-01,  7.3962e-01,\n",
      "          6.1172e-01, -5.0724e-01, -7.0286e-01,  7.3489e-01, -4.8264e-01,\n",
      "          7.1172e-01,  1.0874e-01, -9.0669e-02, -7.7212e-03, -8.5787e-02,\n",
      "          4.5090e-01, -3.3296e-01, -2.1046e-01, -1.4348e-01,  1.6937e-01,\n",
      "          5.3613e-01,  1.7411e-01, -7.5124e-01,  4.4971e-01, -2.6471e-01,\n",
      "          4.6785e-02,  4.3663e-01,  7.7138e-02, -2.5514e-02,  2.9157e-01,\n",
      "         -1.7351e-01,  3.7016e-01, -2.3849e-01, -5.5161e-01, -2.6973e-01,\n",
      "          1.2771e-01,  5.4493e-01, -1.1244e-01,  1.2596e-01, -3.0925e-01,\n",
      "         -7.2663e-01,  2.8653e-01,  1.2299e-01,  3.3829e-02,  8.3745e-01,\n",
      "         -1.7310e-01, -5.3122e-01, -1.7406e-01, -2.1901e-01, -6.8383e-01,\n",
      "         -4.5680e-01, -1.3380e-01, -1.5224e-01,  2.0100e-01,  1.8138e-01,\n",
      "          4.5102e-02, -4.3487e-01, -2.9933e-01,  5.4616e-02, -5.8827e-01,\n",
      "          9.2705e-02,  4.7744e-01,  3.3304e-01, -3.7712e-01,  2.8726e-01,\n",
      "         -1.4877e-01,  2.9455e-01,  3.7806e-01, -3.2460e-01,  4.7290e-01,\n",
      "         -1.0930e-01, -8.4979e-03, -6.2148e-01, -1.8524e-01,  2.2498e-01,\n",
      "          6.7055e-01, -1.1955e-01,  2.4712e-01, -4.5553e-01, -5.8820e-01,\n",
      "          7.6180e-01,  5.3026e-01, -2.4272e-01, -4.4275e-02,  1.6565e-01,\n",
      "          4.4458e-01,  1.8908e-01,  1.2126e-01, -5.2118e-01, -2.4942e-02,\n",
      "          5.6300e-02, -1.2545e-01,  5.5979e-01,  3.2169e-01,  4.7155e-01,\n",
      "          4.5968e-01,  1.6067e-01, -2.2765e-01, -6.2085e-01, -6.2544e-03,\n",
      "          1.5757e-01, -4.6032e-01, -4.1632e-01, -5.0665e-01,  3.9013e-01,\n",
      "          2.7982e-01,  2.7308e-01,  7.4977e-01, -2.3188e-02,  8.9447e-02,\n",
      "          4.8560e-01,  6.4075e-01,  3.8924e-01]]), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (BaseModelOutputWithPooling, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)\n * (Tensor input, name dim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# preds_probs = outputs.prediction.softmax(dim=-1)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     22\u001b[0m predicted_labels\u001b[38;5;241m.\u001b[39mappend(preds\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (BaseModelOutputWithPooling, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)\n * (Tensor input, name dim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Define the ViT model and feature extractor\n",
    "img_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Define the device to train the model on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "predicted_labels=[] \n",
    "submission = {}\n",
    "# Define the testing loop\n",
    "with torch.no_grad():\n",
    "    for image_path in test_data[\"img_path\"]:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        processed_img = img_processor(image, return_tensors='pt', data_format='channels_first').to(device)\n",
    "        outputs = trained_model(**processed_img)\n",
    "        print(outputs)\n",
    "        # preds_probs = outputs.prediction.softmax(dim=-1)\n",
    "        _, preds = torch.softmax(outputs, dim=1)\n",
    "        print(preds.item())\n",
    "        predicted_labels.append(preds.item())\n",
    "        submission[image_path] = preds.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('printing3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9299164e3a9e5e8e000acaafb054c5339c0d681d9daaa5d37bd4f7d064fcfb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
